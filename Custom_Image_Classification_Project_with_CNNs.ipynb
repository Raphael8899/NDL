{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Custom Image Classification Project with CNNs\n\nI wanted this notebook to feel like a real end-to-end experiment instead of just plugging in a model and reporting one number. I tried three approaches on the `tf_flowers` dataset: two transfer-learning models and one custom CNN. The goal is to compare speed, performance, and failure modes in a practical way."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Setup and imports"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nprint('TensorFlow version:', tf.__version__)\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Load dataset (`tf_flowers`) and inspect basic info\n\nAt first I noticed `tf_flowers` comes as one split by default, so I manually split into train/validation/test. This makes it feel closer to a real project workflow."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "(ds_full, ds_info) = tfds.load(\n    'tf_flowers',\n    split='train',\n    shuffle_files=True,\n    with_info=True,\n    as_supervised=True\n)\n\nnum_examples = ds_info.splits['train'].num_examples\nclass_names = ds_info.features['label'].names\nnum_classes = ds_info.features['label'].num_classes\n\nprint(f'Total images: {num_examples}')\nprint(f'Number of classes: {num_classes}')\nprint('Class names:', class_names)\nprint('Task: multi-class image classification')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Manual split: 70% train, 15% val, 15% test\ntrain_pct, val_pct = 70, 15\ntrain_split = f'train[:{train_pct}%]'\nval_split = f'train[{train_pct}%:{train_pct+val_pct}%]'\ntest_split = f'train[{train_pct+val_pct}%:]'\n\nds_train_raw, ds_val_raw, ds_test_raw = tfds.load(\n    'tf_flowers',\n    split=[train_split, val_split, test_split],\n    as_supervised=True,\n    shuffle_files=True\n)\n\nprint('Split sizes (approx):')\nprint('Train:', tf.data.experimental.cardinality(ds_train_raw).numpy())\nprint('Val  :', tf.data.experimental.cardinality(ds_val_raw).numpy())\nprint('Test :', tf.data.experimental.cardinality(ds_test_raw).numpy())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`tf_flowers` has about 3,670 images across 5 flower categories, and image sizes vary quite a bit. So resizing is necessary before batching."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Quick class distribution from the full dataset\nlabel_counts = np.zeros(num_classes, dtype=int)\n\nfor _, label in tfds.as_numpy(ds_full):\n    label_counts[label] += 1\n\nplt.figure(figsize=(8,4))\nplt.bar(class_names, label_counts)\nplt.title('Class distribution in tf_flowers')\nplt.ylabel('Count')\nplt.xticks(rotation=20)\nplt.show()\n\nfor name, count in zip(class_names, label_counts):\n    print(f'{name:10s}: {count}')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Show a grid of sample images\nplt.figure(figsize=(10, 8))\nfor i, (img, label) in enumerate(ds_full.take(12)):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.imshow(img)\n    plt.title(class_names[int(label)])\n    plt.axis('off')\nplt.suptitle('Sample images from tf_flowers', y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Preprocessing and augmentation\n\nI resize everything to `224x224` and normalize to `[0,1]`. For augmentation, I used random flips, rotations, and zoom. This seemed to help generalization in quick experiments because flowers can appear from different angles and distances."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "IMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\naugmentation = tf.keras.Sequential([\n    layers.RandomFlip('horizontal'),\n    layers.RandomRotation(0.15),\n    layers.RandomZoom(0.15),\n], name='augmentation')\n\n\ndef preprocess(image, label):\n    image = tf.image.resize(image, IMG_SIZE)\n    image = tf.cast(image, tf.float32)\n    return image, label\n\n\ndef make_dataset(ds, training=False):\n    ds = ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.shuffle(1000, seed=SEED)\n        ds = ds.map(lambda x, y: (augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return ds\n\ntrain_ds = make_dataset(ds_train_raw, training=True)\nval_ds = make_dataset(ds_val_raw, training=False)\ntest_ds = make_dataset(ds_test_raw, training=False)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Visualize augmentation examples\nsample_images, sample_labels = next(iter(make_dataset(ds_train_raw, training=False).unbatch().batch(6).take(1)))\n\nplt.figure(figsize=(12, 6))\nfor i in range(6):\n    aug_img = augmentation(tf.expand_dims(sample_images[i], axis=0), training=True)[0]\n    ax = plt.subplot(2, 3, i + 1)\n    plt.imshow(tf.clip_by_value(aug_img / 255.0, 0.0, 1.0))\n    plt.title(class_names[int(sample_labels[i])])\n    plt.axis('off')\nplt.suptitle('Augmented image examples', y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Training utilities\n\nI’m using early stopping so training can stop when validation performance stalls. It saves time and usually prevents overfitting from dragging on."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Quick choices I made here: I kept **224x224** because both EfficientNetB0 and MobileNetV2 are commonly used with that size, and it is a good speed/quality tradeoff on this dataset. I used **Adam** because it converges fast in transfer-learning setups. During fine-tuning I dropped the learning rate to avoid destroying pretrained features too quickly. I didn’t use class weights since class imbalance exists but is not extreme here, and I wanted to keep the baseline training setup simple first."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def get_callbacks(model_name):\n    os.makedirs('checkpoints', exist_ok=True)\n    ckpt_path = f'checkpoints/{model_name}.keras'\n    return [\n        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True)\n    ]\n\n\ndef plot_history(history, title='Training history'):\n    hist = history.history\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    axes[0].plot(hist.get('accuracy', []), label='train_acc')\n    axes[0].plot(hist.get('val_accuracy', []), label='val_acc')\n    axes[0].set_title(f'{title} - Accuracy')\n    axes[0].legend()\n\n    axes[1].plot(hist.get('loss', []), label='train_loss')\n    axes[1].plot(hist.get('val_loss', []), label='val_loss')\n    axes[1].set_title(f'{title} - Loss')\n    axes[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef evaluate_model(model, ds, model_name):\n    y_true, y_pred = [], []\n    for x_batch, y_batch in ds:\n        probs = model.predict(x_batch, verbose=0)\n        preds = np.argmax(probs, axis=1)\n        y_true.extend(y_batch.numpy())\n        y_pred.extend(preds)\n\n    acc = accuracy_score(y_true, y_pred)\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n\n    print(f'\\n{model_name} Test Accuracy: {acc:.4f}')\n    print(f'{model_name} Macro F1: {macro_f1:.4f}')\n\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'{model_name} Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.xticks(rotation=20)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n\n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, target_names=class_names, digits=4, zero_division=0))\n\n    return {\n        'accuracy': acc,\n        'macro_f1': macro_f1,\n        'y_true': np.array(y_true),\n        'y_pred': np.array(y_pred)\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Model 1 — EfficientNetB0 (transfer learning)\n\nI started with EfficientNetB0 because it usually gives a nice balance of performance and efficiency.\n\n### Stage 1\nFreeze backbone, train classification head.\n\n### Stage 2\nUnfreeze part of the backbone and fine-tune with a smaller learning rate."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def build_efficientnet_model(num_classes):\n    base = tf.keras.applications.EfficientNetB0(\n        include_top=False,\n        weights='imagenet',\n        input_shape=(224, 224, 3)\n    )\n    base.trainable = False\n\n    inputs = layers.Input(shape=(224, 224, 3))\n    # EfficientNet in Keras expects 0..255 style inputs and handles scaling internally.\n    x = layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, name='efficientnet_preprocess')(inputs)\n    x = base(x, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = models.Model(inputs, outputs)\n    return model, base\n\n\neff_model, eff_base = build_efficientnet_model(num_classes)\neff_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nstart = time.time()\nhist_eff_stage1 = eff_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=5,\n    callbacks=get_callbacks('efficientnet_stage1'),\n    verbose=1\n)\n\n# Fine-tune: unfreeze last ~30 layers\neff_base.trainable = True\nfor layer in eff_base.layers[:-30]:\n    layer.trainable = False\n\neff_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nhist_eff_stage2 = eff_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=4,\n    callbacks=get_callbacks('efficientnet_stage2'),\n    verbose=1\n)\n\neff_time = time.time() - start\nprint(f'EfficientNet total training time: {eff_time:.1f} sec')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "plot_history(hist_eff_stage1, 'EfficientNet Stage 1')\nplot_history(hist_eff_stage2, 'EfficientNet Stage 2')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Model 2 — MobileNetV2 (transfer learning)\n\nMobileNetV2 is lighter, so I expected faster training. I used the same two-stage strategy to keep the comparison fair."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def build_mobilenet_model(num_classes):\n    base = tf.keras.applications.MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_shape=(224, 224, 3)\n    )\n    base.trainable = False\n\n    inputs = layers.Input(shape=(224, 224, 3))\n    x = layers.Lambda(tf.keras.applications.mobilenet_v2.preprocess_input, name='mobilenet_preprocess')(inputs)\n    x = base(x, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = models.Model(inputs, outputs)\n    return model, base\n\n\nmob_model, mob_base = build_mobilenet_model(num_classes)\nmob_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nstart = time.time()\nhist_mob_stage1 = mob_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=5,\n    callbacks=get_callbacks('mobilenet_stage1'),\n    verbose=1\n)\n\nmob_base.trainable = True\nfor layer in mob_base.layers[:-30]:\n    layer.trainable = False\n\nmob_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nhist_mob_stage2 = mob_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=4,\n    callbacks=get_callbacks('mobilenet_stage2'),\n    verbose=1\n)\n\nmob_time = time.time() - start\nprint(f'MobileNet total training time: {mob_time:.1f} sec')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "plot_history(hist_mob_stage1, 'MobileNet Stage 1')\nplot_history(hist_mob_stage2, 'MobileNet Stage 2')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) Model 3 — Custom CNN from scratch\n\nI decided not to make this network huge because I wanted training to stay quick. I still added batch norm + dropout to stabilize training and reduce overfitting."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def build_custom_cnn(num_classes):\n    inputs = layers.Input(shape=(224, 224, 3))\n\n    x = layers.Rescaling(1./255, name='custom_rescale')(inputs)\n    x = layers.Conv2D(32, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.MaxPooling2D()(x)\n\n    x = layers.Conv2D(64, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.MaxPooling2D()(x)\n\n    x = layers.Conv2D(128, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    return models.Model(inputs, outputs)\n\n\ncustom_model = build_custom_cnn(num_classes)\ncustom_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n                     loss='sparse_categorical_crossentropy',\n                     metrics=['accuracy'])\n\nstart = time.time()\nhist_custom = custom_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=12,\n    callbacks=get_callbacks('custom_cnn'),\n    verbose=1\n)\ncustom_time = time.time() - start\nprint(f'Custom CNN total training time: {custom_time:.1f} sec')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "plot_history(hist_custom, 'Custom CNN')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Evaluation results\n\nNow I evaluate each model on the held-out test split using accuracy, macro F1, confusion matrix, and full classification report."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "results = {}\nresults['EfficientNetB0'] = evaluate_model(eff_model, test_ds, 'EfficientNetB0')\nresults['MobileNetV2'] = evaluate_model(mob_model, test_ds, 'MobileNetV2')\nresults['CustomCNN'] = evaluate_model(custom_model, test_ds, 'CustomCNN')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "comparison_df = {\n    'Model': ['EfficientNetB0', 'MobileNetV2', 'CustomCNN'],\n    'Val_Accuracy': [\n        max(hist_eff_stage2.history['val_accuracy']) if 'val_accuracy' in hist_eff_stage2.history else np.nan,\n        max(hist_mob_stage2.history['val_accuracy']) if 'val_accuracy' in hist_mob_stage2.history else np.nan,\n        max(hist_custom.history['val_accuracy']) if 'val_accuracy' in hist_custom.history else np.nan,\n    ],\n    'Test_Accuracy': [results['EfficientNetB0']['accuracy'], results['MobileNetV2']['accuracy'], results['CustomCNN']['accuracy']],\n    'Macro_F1': [results['EfficientNetB0']['macro_f1'], results['MobileNetV2']['macro_f1'], results['CustomCNN']['macro_f1']],\n    'Train_Time_sec': [eff_time, mob_time, custom_time],\n}\n\ndef overfit_note(history_obj):\n    h = history_obj.history\n    if 'accuracy' not in h or 'val_accuracy' not in h:\n        return 'Not enough logs'\n    gap = max(h['accuracy']) - max(h['val_accuracy'])\n    if gap < 0.03:\n        return 'Low overfitting'\n    if gap < 0.08:\n        return 'Moderate overfitting'\n    return 'Higher overfitting'\n\nimport pandas as pd\ncomp = pd.DataFrame(comparison_df)\ncomp['Overfitting_Behavior'] = [\n    overfit_note(hist_eff_stage2),\n    overfit_note(hist_mob_stage2),\n    overfit_note(hist_custom)\n]\ncomp = comp.sort_values(by='Test_Accuracy', ascending=False)\ncomp\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Error analysis\n\nInterestingly, even when overall accuracy is good, some classes still get mixed up because flower shapes/colors overlap. I’ll visualize a few correct and wrong predictions from the best model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "best_model_name = comp.iloc[0]['Model']\nname_to_model = {\n    'EfficientNetB0': eff_model,\n    'MobileNetV2': mob_model,\n    'CustomCNN': custom_model\n}\nbest_model = name_to_model[best_model_name]\nprint('Best model based on test accuracy:', best_model_name)\n\n# Gather test images/preds for display\nall_images, all_true, all_pred = [], [], []\nfor x_batch, y_batch in test_ds:\n    probs = best_model.predict(x_batch, verbose=0)\n    preds = np.argmax(probs, axis=1)\n    all_images.append(x_batch.numpy())\n    all_true.append(y_batch.numpy())\n    all_pred.append(preds)\n\nall_images = np.concatenate(all_images, axis=0)\nall_true = np.concatenate(all_true, axis=0)\nall_pred = np.concatenate(all_pred, axis=0)\n\ncorrect_idx = np.where(all_true == all_pred)[0]\nwrong_idx = np.where(all_true != all_pred)[0]\n\nplt.figure(figsize=(12, 5))\nfor i, idx in enumerate(correct_idx[:6]):\n    ax = plt.subplot(2, 3, i+1)\n    plt.imshow(all_images[idx] / 255.0)\n    plt.title(f'T:{class_names[all_true[idx]]} | P:{class_names[all_pred[idx]]}')\n    plt.axis('off')\nplt.suptitle('Correct predictions', y=1.03)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12, 5))\nfor i, idx in enumerate(wrong_idx[:6]):\n    ax = plt.subplot(2, 3, i+1)\n    plt.imshow(all_images[idx] / 255.0)\n    plt.title(f'T:{class_names[all_true[idx]]} | P:{class_names[all_pred[idx]]}')\n    plt.axis('off')\nplt.suptitle('Wrong predictions', y=1.03)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Short observations from errors\n\nFrom the wrong examples, I noticed visually similar classes are the main source of confusion (especially when the flower occupies only part of the image or lighting is odd). The model also struggles on samples where background colors dominate and petals are less clear."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10) Model comparison discussion\n\n- **Fastest training:** usually MobileNetV2 in my runs, which matches its lightweight design.\n- **Best generalization:** depends on the run, but one of the transfer-learning models usually wins on macro F1.\n- **Most overfitting risk:** the custom CNN can overfit faster if I push epochs too high.\n- **Overall pick:** I look at validation accuracy + test accuracy + macro F1 together, then check overfitting behavior before deciding.\n\nDue to time constraints GradCAM was not implemented, but it would help visualize which image regions influenced predictions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11) Conclusion\n\nThis was a useful comparison. Transfer learning gave strong results quickly, while the custom CNN was good as a baseline but needed more tuning to match pretrained backbones. If I had more time, I’d do stronger hyperparameter tuning and maybe class-balanced augmentation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12) LLM usage note\n\n“I used an LLM mainly to help structure the notebook and debug some TensorFlow issues. The modelling decisions and interpretation were my own.”"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}